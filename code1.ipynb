{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0b068e6-6cf2-4df4-8c32-630b1f99e989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f95a10b3-59d1-4f5a-bf29-cf0ee51c8330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): Sequential(\n",
       "      (encoder_layer_0): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_1): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_2): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_3): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_4): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_5): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_6): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_7): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_8): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_9): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_10): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_11): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (heads): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the pretrained weights\n",
    "pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
    "\n",
    "#initializing the model with the weights\n",
    "pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)\n",
    "\n",
    "for param in pretrained_vit.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "class_names = ['Bacterialblight','Blast','Brownspot','Tungro']\n",
    "pretrained_vit.heads = nn.Linear(in_features = 768 , out_features = len(class_names)).to(device)\n",
    "pretrained_vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b06ed0c-921a-4476-a4b3-9dadcd192830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassification(\n",
       "    crop_size=[224]\n",
       "    resize_size=[256]\n",
       "    mean=[0.485, 0.456, 0.406]\n",
       "    std=[0.229, 0.224, 0.225]\n",
       "    interpolation=InterpolationMode.BILINEAR\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_vit_transforms = pretrained_vit_weights.transforms()\n",
    "pretrained_vit_transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be54775b-2113-4117-aacc-fff9b1452a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    pretrained_vit_weights.transforms()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03068342-0c06-48a8-87f3-8d5a71e13dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Define the transformations for the training data\n",
    "train_transforms = transform\n",
    "\n",
    "# Define the transformations for the test data (only basic transformations)\n",
    "test_transforms = pretrained_vit_transforms\n",
    "\n",
    "# Path to your dataset\n",
    "data_dir = 'data'\n",
    "\n",
    "# Load the dataset\n",
    "full_dataset = datasets.ImageFolder(data_dir)\n",
    "\n",
    "# Split the dataset into train and test sets (80% train, 20% test)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "valid_size = int(0.5*len(test_dataset))\n",
    "test_size = len(test_dataset)-valid_size\n",
    "valid_dataset, test_dataset = random_split(test_dataset, [valid_size, test_size])\n",
    "valid_dataset.dataset.transform = test_transforms\n",
    "test_dataset.dataset.transform = test_transforms\n",
    "\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82b1ed00-206f-40dd-b891-04898e2e78db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a4e576a-401a-4e0f-94ea-ce5d6f9a2bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.dataset.transform = test_transforms\n",
    "no_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "train_dataset.dataset.transform = train_transforms\n",
    "yes_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "195a8bc6-4176-4bcb-babb-156f19b24096",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "dataset1 = no_dataloader.dataset\n",
    "dataset2 = yes_dataloader.dataset\n",
    "\n",
    "# Combine datasets\n",
    "combined_dataset = ConcatDataset([dataset1, dataset2])\n",
    "\n",
    "# Create a new DataLoader for the combined dataset\n",
    "new_dataloader = DataLoader(combined_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18761e10-8139-4a52-ab1a-f939d77e7038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "297"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7c2ec3e-1651-4a47-a6fb-07794727a788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srikar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 297/297 [1:23:43<00:00, 16.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:\n",
      "Train Loss: 0.2710 | Train Accuracy: 92.21%\n",
      "Test Loss: 0.1200 | Test Accuracy: 97.30%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 297/297 [1:19:42<00:00, 16.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:\n",
      "Train Loss: 0.0859 | Train Accuracy: 98.23%\n",
      "Test Loss: 0.0678 | Test Accuracy: 98.82%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 297/297 [1:40:18<00:00, 20.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:\n",
      "Train Loss: 0.0529 | Train Accuracy: 99.17%\n",
      "Test Loss: 0.0465 | Test Accuracy: 99.16%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 297/297 [1:39:30<00:00, 20.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:\n",
      "Train Loss: 0.0377 | Train Accuracy: 99.46%\n",
      "Test Loss: 0.0337 | Test Accuracy: 99.66%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 297/297 [1:54:28<00:00, 23.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:\n",
      "Train Loss: 0.0290 | Train Accuracy: 99.59%\n",
      "Test Loss: 0.0262 | Test Accuracy: 99.66%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pretrained_vit.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(pretrained_vit.parameters(), lr=1e-3)\n",
    "\n",
    "# Number of epochs\n",
    "epochs = 5\n",
    "\n",
    "# Training and validation loop\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    pretrained_vit.train()\n",
    "    train_loss, train_correct = 0, 0\n",
    "    for images, labels in tqdm(new_dataloader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = pretrained_vit(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track accuracy and loss\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss /= len(new_dataloader.dataset)\n",
    "    train_acc = train_correct / len(new_dataloader.dataset)\n",
    "\n",
    "    # Validation phase\n",
    "    pretrained_vit.eval()\n",
    "    valid_loss, valid_correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valid_dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = pretrained_vit(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            # Track accuracy and loss\n",
    "            valid_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            valid_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    valid_loss /= len(valid_dataloader.dataset)\n",
    "    valid_acc = valid_correct / len(valid_dataloader.dataset)\n",
    "\n",
    "    # Print statistics\n",
    "    print(f'Epoch {epoch+1}/{epochs}:')\n",
    "    print(f'Train Loss: {train_loss:.4f} | Train Accuracy: {train_acc*100:.2f}%')\n",
    "    print(f'Test Loss: {valid_loss:.4f} | Test Accuracy: {valid_acc*100:.2f}%\\n')\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(pretrained_vit.state_dict(), 'vit_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04f9221b-0e15-4819-b0d3-38a446f02e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement scikit-learn (from versions: none)\n",
      "ERROR: No matching distribution found for scikit-learn\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1609c587-a355-4409-9ee4-2c9b587337bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m f1_score, recall_score, precision_score\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Move the model to evaluation mode\u001b[39;00m\n\u001b[0;32m      4\u001b[0m pretrained_vit\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "\n",
    "# Move the model to evaluation mode\n",
    "pretrained_vit.eval()\n",
    "\n",
    "test_loss, test_correct = 0, 0\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "# Disable gradient calculations for validation/testing\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = pretrained_vit(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Track accuracy and loss\n",
    "        test_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Store all predictions and labels\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate test loss and accuracy\n",
    "test_loss /= len(test_dataloader.dataset)\n",
    "test_acc = test_correct / len(test_dataloader.dataset)\n",
    "\n",
    "# Calculate F1 score, recall, and precision\n",
    "f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "# Output the results\n",
    "print(f'Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc*100:.2f}%')\n",
    "print(f'F1 Score: {f1:.4f} | Recall: {recall:.4f} | Precision: {precision:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4289b248-8ca3-4c2d-af4f-18f09da82440",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pretrained_vit,'new_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e85451a-2197-477b-824d-bc063ee93799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: Tungro\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def load_image_from_folder(image_path):\n",
    "    # Open image and apply transformations\n",
    "    image = Image.open(image_path)\n",
    "    transformed_image = test_transforms(image).unsqueeze(0)  # Add batch dimension\n",
    "    return transformed_image\n",
    "def predict_class(image_tensor):\n",
    "    # Move the tensor to the appropriate device\n",
    "    image_tensor = image_tensor.to(device)\n",
    "    \n",
    "    # Pass through model and get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = pretrained_vit(image_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    return predicted.item()\n",
    "image_tensor = load_image_from_folder('data/Tungro/TUNGRO2_029.jpg')\n",
    "\n",
    "# Predict class\n",
    "predicted_class = predict_class(image_tensor)\n",
    "\n",
    "print(f\"Predicted class: {class_names[predicted_class]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f70fdc7e-6b0d-4339-af05-7c23a2867709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "297"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "869fc5b4-84be-444c-bb42-92c9ccb8a817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.4561\n",
      "Epoch [2/5], Loss: 0.2331\n",
      "Epoch [3/5], Loss: 0.1877\n",
      "Epoch [4/5], Loss: 0.1672\n",
      "Epoch [5/5], Loss: 0.1533\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "#loading and resnet model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "\n",
    "# Define the ResNet model with pretrained weights\n",
    "resnet_model = models.resnet50(pretrained=True)\n",
    "\n",
    "# Freeze the weights of all layers except the final layer\n",
    "for param in resnet_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the final fully connected layer (fc) for the new task\n",
    "# Assuming you're classifying into 'num_classes' categories\n",
    "num_classes = 4  # Change this to your number of output classes\n",
    "resnet_model.fc = nn.Linear(resnet_model.fc.in_features, num_classes)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet_model = resnet_model.to(device)\n",
    "\n",
    "# Define loss function and optimizer (only optimizing the final layer)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(resnet_model.fc.parameters(), lr=0.001)\n",
    "\n",
    "# Assuming you have a DataLoader for training\n",
    "# Example: dataloader with batch size of 32 and image size 224x224\n",
    "# train_dataloader is assumed to be defined\n",
    "# train_dataloader = DataLoader(your_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 5  # You can adjust this\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    resnet_model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in new_dataloader:\n",
    "        # Move inputs and labels to the device (GPU or CPU)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = resnet_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize only the final layer\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(new_dataloader):.4f}')\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9c47560-580f-4342-b8e9-790937cd2d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(resnet_model,'resnet_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e5d2857-36f2-44bd-af86-11d9e7eaa926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0961 | Test Accuracy: 96.30%\n",
      "F1 Score: 0.9630 | Recall: 0.9630 | Precision: 0.9635\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "\n",
    "# Move the model to evaluation mode\n",
    "resnet_model.eval()\n",
    "\n",
    "test_loss, test_correct = 0, 0\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# Disable gradient calculations for validation/testing\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = resnet_model(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Track accuracy and loss\n",
    "        test_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Store all predictions and labels\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate test loss and accuracy\n",
    "test_loss /= len(test_dataloader.dataset)\n",
    "test_acc = test_correct / len(test_dataloader.dataset)\n",
    "\n",
    "# Calculate F1 score, recall, and precision\n",
    "f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "# Output the results\n",
    "print(f'Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc*100:.2f}%')\n",
    "print(f'F1 Score: {f1:.4f} | Recall: {recall:.4f} | Precision: {precision:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51c25fdc-6dae-45eb-9b3d-2c72d104691a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|███████████████████████████████████████████████████████████████| 297/297 [2:55:48<00:00, 35.52s/batch, loss=0.0401, accuracy=0.955]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.1320, Accuracy: 0.9552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████████████████████████████████████████████████████████| 297/297 [1:58:35<00:00, 23.96s/batch, loss=0.00033, accuracy=0.991]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Loss: 0.0264, Accuracy: 0.9909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████████████████████████████████████████████████████████| 297/297 [1:56:30<00:00, 23.54s/batch, loss=0.00102, accuracy=0.996]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Loss: 0.0101, Accuracy: 0.9963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|████████████████████████████████████████████████████████████████| 297/297 [1:56:14<00:00, 23.48s/batch, loss=0.343, accuracy=0.989]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Loss: 0.0274, Accuracy: 0.9890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████████████████████████████████████████████████████████| 297/297 [1:56:11<00:00, 23.47s/batch, loss=0.00862, accuracy=0.996]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Loss: 0.0111, Accuracy: 0.9964\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from tqdm import tqdm  # tqdm for progress bar\n",
    "\n",
    "# Define the ViT model with pretrained weights\n",
    "vit_model = models.vit_b_16(pretrained=True)\n",
    "\n",
    "# Define the ResNet model with pretrained weights\n",
    "resnet_model = models.resnet50(pretrained=True)\n",
    "\n",
    "# Freeze the weights of all layers in both models except the final layers\n",
    "for param in vit_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in resnet_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the final layers of both models\n",
    "num_classes = 4  # Adjust to your number of classes\n",
    "vit_model.heads = nn.Identity()  # Remove classification head from ViT\n",
    "resnet_model.fc = nn.Identity()  # Remove fully connected layer from ResNet\n",
    "\n",
    "# Combined Model with Feedforward Network\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, vit, resnet):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.vit = vit\n",
    "        self.resnet = resnet\n",
    "        # Fully connected layer after concatenating features\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(768 + 2048, 512),  # Concatenate ViT and ResNet features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_classes)  # Output layer for classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract features from ViT and ResNet\n",
    "        vit_features = self.vit(x)\n",
    "        resnet_features = self.resnet(x)\n",
    "        \n",
    "        # Concatenate the features\n",
    "        combined_features = torch.cat((vit_features, resnet_features), dim=1)\n",
    "        \n",
    "        # Pass through the feedforward network\n",
    "        output = self.fc(combined_features)\n",
    "        return output\n",
    "\n",
    "# Initialize combined model\n",
    "combined_model = CombinedModel(vit_model, resnet_model)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "combined_model = combined_model.to(device)\n",
    "\n",
    "# Define loss function and optimizer (training only the FC layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(combined_model.fc.parameters(), lr=0.001)\n",
    "\n",
    "# Training setup\n",
    "num_epochs = 5  # Adjust to your preference\n",
    "\n",
    "# Training Loop with tqdm progress bar\n",
    "for epoch in range(num_epochs):\n",
    "    combined_model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Using tqdm to add a progress bar to the dataloader\n",
    "    with tqdm(total=len(new_dataloader), desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch') as pbar:\n",
    "        for batch_idx, (inputs, labels) in enumerate(new_dataloader):\n",
    "            # Move inputs and labels to the device (GPU or CPU)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = combined_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track running loss and accuracy\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            # Update tqdm progress bar\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({'loss': loss.item(), 'accuracy': total_correct/total_samples})\n",
    "\n",
    "    # Print final epoch stats\n",
    "    epoch_loss = running_loss / len(new_dataloader)\n",
    "    epoch_accuracy = total_correct / total_samples\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6df2ee53-462e-4363-85fc-979d2ed3e5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(combined_model,'combined_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ddcb56a-0ad0-432d-8e0a-a18e2e7bf438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0006 | Test Accuracy: 100.00%\n",
      "F1 Score: 1.0000 | Recall: 1.0000 | Precision: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "\n",
    "# Move the model to evaluation mode\n",
    "combined_model.eval()\n",
    "\n",
    "test_loss, test_correct = 0, 0\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# Disable gradient calculations for validation/testing\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = combined_model(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Track accuracy and loss\n",
    "        test_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Store all predictions and labels\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate test loss and accuracy\n",
    "test_loss /= len(test_dataloader.dataset)\n",
    "test_acc = test_correct / len(test_dataloader.dataset)\n",
    "\n",
    "# Calculate F1 score, recall, and precision\n",
    "f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "# Output the results\n",
    "print(f'Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc*100:.2f}%')\n",
    "print(f'F1 Score: {f1:.4f} | Recall: {recall:.4f} | Precision: {precision:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe134ce5-9b4e-4489-83e4-ed027d77d227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0009 | Test Accuracy: 100.00%\n",
      "F1 Score: 1.0000 | Recall: 1.0000 | Precision: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "\n",
    "# Move the model to evaluation mode\n",
    "combined_model.eval()\n",
    "\n",
    "test_loss, test_correct = 0, 0\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# Disable gradient calculations for validation/testing\n",
    "with torch.no_grad():\n",
    "    for images, labels in valid_dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = combined_model(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Track accuracy and loss\n",
    "        test_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Store all predictions and labels\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate test loss and accuracy\n",
    "test_loss /= len(valid_dataloader.dataset)\n",
    "test_acc = test_correct / len(valid_dataloader.dataset)\n",
    "\n",
    "# Calculate F1 score, recall, and precision\n",
    "f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "# Output the results\n",
    "print(f'Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc*100:.2f}%')\n",
    "print(f'F1 Score: {f1:.4f} | Recall: {recall:.4f} | Precision: {precision:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
